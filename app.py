import streamlit as st  # å¯¼å…¥ Streamlit åº“ï¼Œç”¨äºå¿«é€Ÿæ„å»º Web äº¤äº’ç•Œé¢
import joblib           # å¯¼å…¥ joblibï¼Œç”¨äºåŠ è½½ä¹‹å‰ä¿å­˜çš„ .pkl æ¨¡å‹æ–‡ä»¶
import jieba            # å¯¼å…¥ jiebaï¼Œç”¨äºå¯¹ç”¨æˆ·è¾“å…¥çš„å®æ—¶æ–‡æœ¬è¿›è¡Œåˆ†è¯
import re               # å¯¼å…¥æ­£åˆ™æ¨¡å—ï¼Œç”¨äºæ¸…æ´—ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
import numpy as np      # å¯¼å…¥ numpyï¼Œç”¨äºå¤„ç†æ•°å€¼ç»Ÿè®¡ç‰¹å¾
import os               # å¯¼å…¥ osï¼Œç”¨äºæ£€æŸ¥æœ¬åœ°ç£ç›˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
from scipy.sparse import hstack, csr_matrix  # å¯¼å…¥çŸ©é˜µå¤„ç†å·¥å…·ï¼Œç”¨äºæ‹¼æ¥ç‰¹å¾

# --- 0. é¡µé¢å…¨å±€é…ç½® ---
# è®¾ç½®ç½‘é¡µçš„æ ‡é¢˜å’Œå¸ƒå±€æ¨¡å¼ï¼ˆwide ä¸ºå®½å±æ¨¡å¼ï¼Œçœ‹èµ·æ¥æ›´åƒåå°ç³»ç»Ÿï¼‰
st.set_page_config(page_title="åƒåœ¾çŸ­ä¿¡è¯†åˆ«ç³»ç»Ÿ", layout="wide")


# --- 1. åŠ è½½æ¨¡å‹å’Œå·¥å…· ---
# ä½¿ç”¨ @st.cache_resource è£…é¥°å™¨ï¼š
# è¯¥åŠŸèƒ½å¯ä»¥ç¡®ä¿æ¨¡å‹åªåœ¨ç¬¬ä¸€æ¬¡å¯åŠ¨æ—¶åŠ è½½åˆ°å†…å­˜ï¼Œé¿å…æ¯æ¬¡ç”¨æˆ·ç‚¹å‡»æŒ‰é’®éƒ½é‡æ–°è¯»å–æ–‡ä»¶ï¼Œå¤§å¹…æå‡æ€§èƒ½ã€‚
@st.cache_resource
def load_resources():
    # ä»æœ¬åœ°ç£ç›˜åŠ è½½è®­ç»ƒè„šæœ¬ (pro.py) ç”Ÿæˆçš„äº”ä¸ªæ ¸å¿ƒç»„ä»¶
    model = joblib.load('spam_model.pkl')    # åŠ è½½è½¯æŠ•ç¥¨é›†æˆæ¨¡å‹
    vec_word = joblib.load('tfidf_word.pkl') # åŠ è½½è¯çº§ TF-IDF å‘é‡åŒ–å™¨
    vec_char = joblib.load('tfidf_char.pkl') # åŠ è½½å­—çº§ TF-IDF å‘é‡åŒ–å™¨
    scaler = joblib.load('scaler.pkl')       # åŠ è½½æ•°å€¼ç‰¹å¾ç¼©æ”¾å™¨
    return model, vec_word, vec_char, scaler

# æ‰§è¡ŒåŠ è½½å‡½æ•°ï¼Œè·å–å…¨å±€å…±äº«çš„èµ„æº
model, vec_word, vec_char, scaler = load_resources()


# --- 2. é¢„å¤„ç†å‡½æ•° ---
# è¯¥å‡½æ•°å¿…é¡»ä¸ pro.py ä¸­çš„ç‰¹å¾å¤„ç†é€»è¾‘å®Œå…¨ä¸€è‡´ï¼Œå¦åˆ™æ¨¡å‹æ— æ³•è¯†åˆ«è¾“å…¥æ•°æ®
def preprocess_input(text, vec_word, vec_char, scaler):
    # A. æ–‡æœ¬æ¸…æ´—ä¸åˆ†è¯
    # å®šä¹‰åœç”¨è¯ï¼Œè¿‡æ»¤æ‰æ— æ„ä¹‰çš„è¾…åŠ©è¯
    stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€'}
    # æ­£åˆ™æ¸…æ´—ï¼šåªä¿ç•™ä¸­æ–‡ã€æ•°å­—å’Œè‹±æ–‡ï¼Œå‰”é™¤è¡¨æƒ…ç¬¦å·åŠç‰¹æ®Šæ ‡ç‚¹
    clean_text = re.sub(r'[^\u4e00-\u9fff0-9a-zA-Z]', ' ', str(text))
    # ä½¿ç”¨ jieba è¿›è¡Œåˆ†è¯å¹¶æ‹¼æ¥
    words = " ".join([w for w in jieba.cut(clean_text) if w not in stopwords and w.strip()])

    # B. ç‰¹å¾è½¬æ¢
    # å°†æ¸…æ´—åçš„å­—ç¬¦ä¸²åˆ†åˆ«è¾“å…¥è¯çº§å’Œå­—çº§å‘é‡åŒ–å™¨ï¼ˆæ³¨æ„è¾“å…¥éœ€è¦æ˜¯åˆ—è¡¨æ ¼å¼ï¼‰
    x_w = vec_word.transform([words])
    x_c = vec_char.transform([words])

    # C. æå–ç»Ÿè®¡ç‰¹å¾
    # çŸ­ä¿¡ä¸­å¸¸è§çš„æ•æ„Ÿè¯åº“
    keywords = ['å…è´¹', 'çº¢åŒ…', 'é“¾æ¥', 'åŠ å¾®', 'é€€è®¢', 'ä¸­å¥–', 'ç§¯åˆ†', 'å›T']
    # è®¡ç®—ï¼š[æ€»é•¿åº¦, æ•°å­—ä¸ªæ•°, æ•æ„Ÿè¯å‘½ä¸­æ¬¡æ•°]
    stats = np.array([[len(text), len(re.findall(r'\d+', text)), sum(text.count(w) for w in keywords)]])
    # ä½¿ç”¨è®­ç»ƒæ—¶çš„ç¼©æ”¾æ ‡å‡†å¯¹å½“å‰æ•°æ®è¿›è¡Œç¼©æ”¾
    x_s = scaler.transform(stats)

    # D. ç‰¹å¾æ‹¼æ¥
    # å°†è¯ç‰¹å¾ã€å­—ç‰¹å¾ã€ç»Ÿè®¡ç‰¹å¾åˆå¹¶ä¸ºä¸€ä¸ªç¨€ç–çŸ©é˜µï¼Œä½œä¸ºæ¨¡å‹è¾“å…¥
    return hstack([x_w, x_c, csr_matrix(x_s)])


# --- 3. ç•Œé¢å¸ƒå±€ ---
# é¡µé¢å¤§æ ‡é¢˜
st.title("ğŸ›¡ï¸ ä¸­æ–‡åƒåœ¾çŸ­ä¿¡æ™ºèƒ½è¯†åˆ«ç³»ç»Ÿ")
st.markdown("---") # æ’å…¥åˆ†å‰²çº¿

# é‡‡ç”¨ä¸¤åˆ—å¸ƒå±€ï¼Œå·¦ä¾§ä¸ºè¾“å…¥æ£€æµ‹åŒºï¼Œå³ä¾§ä¸ºç»“æœåˆ†æåŒº
col1, col2 = st.columns([1, 1])

# --- å·¦ä¾§ï¼šåœ¨çº¿æ£€æµ‹åŒº ---
with col1:
    st.subheader("ğŸ” åœ¨çº¿æ£€æµ‹")
    # åˆ›å»ºå¤šè¡Œæ–‡æœ¬è¾“å…¥æ¡†ï¼Œé«˜åº¦ 150 åƒç´ 
    input_text = st.text_area("è¯·è¾“å…¥çŸ­ä¿¡å†…å®¹ï¼š", height=150, placeholder="åœ¨æ­¤è¾“å…¥éœ€è¦æ£€æµ‹çš„çŸ­ä¿¡æ–‡æœ¬...")

    # ç‚¹å‡»è¯†åˆ«æŒ‰é’®è§¦å‘é€»è¾‘
    if st.button("å¼€å§‹è¯†åˆ«", type="primary"):
        if input_text.strip(): # ç¡®ä¿è¾“å…¥ä¸ä¸ºç©º
            # è°ƒç”¨é¢„å¤„ç†å‡½æ•°å°†æ–‡å­—è½¬ä¸ºç‰¹å¾å‘é‡
            features = preprocess_input(input_text, vec_word, vec_char, scaler)

            # è·å–é¢„æµ‹ç±»åˆ«ï¼ˆ0 æˆ– 1ï¼‰
            prediction = model.predict(features)[0]
            # è·å–æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ç™¾åˆ†æ¯”
            probabilities = model.predict_proba(features)[0]

            # æ ¹æ®é¢„æµ‹ç»“æœæ˜¾ç¤ºä¸åŒé¢œè‰²æ ·å¼çš„æç¤ºæ¡†
            if prediction == 1:
                st.error(f"### æ£€æµ‹ç»“æœï¼šã€åƒåœ¾çŸ­ä¿¡ã€‘") # çº¢è‰²èƒŒæ™¯
            else:
                st.success(f"### æ£€æµ‹ç»“æœï¼šã€æ­£å¸¸çŸ­ä¿¡ã€‘") # ç»¿è‰²èƒŒæ™¯

            # æ¦‚ç‡å¯è§†åŒ–å±•ç¤º
            st.write("#### ç½®ä¿¡åº¦åˆ†æï¼š")
            spam_proba = probabilities[1] # è·å–åƒåœ¾ç±»åˆ«çš„æ¦‚ç‡
            # æ˜¾ç¤ºä¸€ä¸ªåŠ¨æ€è¿›åº¦æ¡ï¼Œç›´è§‚åæ˜ åˆ¤å®šçš„â€œåšå®šç¨‹åº¦â€
            st.progress(spam_proba)
            st.write(f"åƒåœ¾çŸ­ä¿¡æ¦‚ç‡: {spam_proba * 100:.2f}%")
            st.write(f"æ­£å¸¸çŸ­ä¿¡æ¦‚ç‡: {probabilities[0] * 100:.2f}%")
        else:
            # è¾“å…¥ä¸ºç©ºæ—¶çš„è­¦å‘Šæç¤º
            st.warning("è¯·è¾“å…¥çŸ­ä¿¡å†…å®¹åå†ç‚¹å‡»è¯†åˆ«ã€‚")

# --- å³ä¾§ï¼šæ¨¡å‹è¡¨ç°å±•ç¤ºåŒº ---
with col2:
    st.subheader("ğŸ“Š æ¨¡å‹è®­ç»ƒè¡¨ç°")
    # è®¾ç½®è®­ç»ƒç»“æœå›¾ç‰‡è·¯å¾„
    img_path = 'model_results.png'
    # é€»è¾‘åˆ¤æ–­ï¼šå¦‚æœæœ¬åœ°æœ‰å›¾ç‰‡åˆ™æ˜¾ç¤ºï¼Œæ²¡æœ‰åˆ™æç¤ºå…ˆè®­ç»ƒ
    if os.path.exists(img_path):
        # æ˜¾ç¤ºå›¾ç‰‡å¹¶è‡ªé€‚åº”å®¹å™¨å®½åº¦
        st.image(img_path, caption="è®­ç»ƒé›†è¯„ä¼°ï¼šæ··æ·†çŸ©é˜µä¸æ•°æ®åˆ†å¸ƒå›¾", use_container_width=True)
    else:
        st.info("ğŸ’¡ è®­ç»ƒå›¾è¡¨ model_results.png å°šæœªç”Ÿæˆã€‚è¯·å…ˆè¿è¡Œä¸€é pro.pyã€‚")

    # åº•éƒ¨æ˜¾ç¤ºæ¨¡å‹çš„æ ¸å¿ƒæ¶æ„å‚æ•°ï¼Œæ–¹ä¾¿æŸ¥é˜…
    st.markdown("""
    **æ¨¡å‹ä¿¡æ¯ï¼š**
    - ç®—æ³•ï¼šVotingClassifier (LR + SVC + NaiveBayes)
    - æŠ•ç¥¨æ¨¡å¼ï¼šè½¯æŠ•ç¥¨ (Soft Voting)
    - ç‰¹å¾ï¼šTF-IDF (Word & Char) + ç»Ÿè®¡ç‰¹å¾
    """)