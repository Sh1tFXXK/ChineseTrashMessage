import os  # å¯¼å…¥æ“ä½œç³»ç»Ÿæ¥å£æ¨¡å—ï¼Œç”¨äºå¤„ç†è·¯å¾„å’Œç¯å¢ƒå˜é‡
import matplotlib  # å¯¼å…¥ç»˜å›¾åº“åŸºç±»

# ã€å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶æŒ‡å®šåç«¯ã€‘
# 'Agg' æ˜¯ä¸€ä¸ªéäº¤äº’å¼åç«¯ï¼Œåªè´Ÿè´£å°†å›¾è¡¨æ¸²æŸ“åˆ°æ–‡ä»¶ï¼Œè€Œä¸å°è¯•æ‰“å¼€çª—å£ã€‚
# è¿™èƒ½å½»åº•è§£å†³åœ¨ Windows/Conda ç¯å¢ƒä¸‹è¿è¡Œç»˜å›¾ä»£ç æ—¶ï¼Œç¨‹åºå¡æ­»æˆ–é™é»˜å´©æºƒçš„é—®é¢˜ã€‚
matplotlib.use('Agg')  # è®¾ç½® matplotlib ä½¿ç”¨ Agg åç«¯ï¼Œç¡®ä¿åœ¨æ— ç•Œé¢ç¯å¢ƒä¸‹ç¨³å®šä¿å­˜å›¾ç‰‡
import matplotlib.pyplot as plt  # å¯¼å…¥ç»˜å›¾ç»ˆç«¯æ¥å£

# è§£å†³ Intel MKL åº“åœ¨ç‰¹å®šç¯å¢ƒä¸‹å¤šæ¬¡åˆå§‹åŒ–çš„å†²çªé—®é¢˜ï¼Œé˜²æ­¢ç¨‹åºæŠ¥é”™é€€å‡º
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"  # å¼ºåˆ¶å…è®¸ OpenMP è¿è¡Œæ—¶åº“é‡å¤åˆå§‹åŒ–

import pandas as pd  # å¯¼å…¥æ•°æ®åˆ†æåº“ï¼Œç”¨äºå¤„ç†è¡¨æ ¼æ•°æ®
import numpy as np  # å¯¼å…¥æ•°å€¼è®¡ç®—åº“ï¼Œç”¨äºçŸ©é˜µè¿ç®—
import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—ï¼Œç”¨äºæ–‡æœ¬æ¸…æ´—
import jieba  # å¯¼å…¥ä¸­æ–‡åˆ†è¯åº“ï¼Œç”¨äºåˆ‡åˆ†ä¸­æ–‡å¥å­
import joblib  # å¯¼å…¥æ¨¡å‹ä¿å­˜åº“ï¼Œç”¨äºåºåˆ—åŒ– Python å¯¹è±¡
from sklearn.feature_extraction.text import TfidfVectorizer  # å¯¼å…¥ TF-IDF å‘é‡åŒ–å·¥å…·
from sklearn.naive_bayes import ComplementNB  # å¯¼å…¥ä¸“é—¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„æœ´ç´ è´å¶æ–¯æ¨¡å‹
from sklearn.linear_model import LogisticRegression  # å¯¼å…¥é€»è¾‘å›å½’æ¨¡å‹
from sklearn.svm import SVC  # ä½¿ç”¨æ”¯æŒæ¦‚ç‡è¾“å‡ºçš„æ ‡å‡†æ”¯æŒå‘é‡æœº (SVC)
from sklearn.ensemble import VotingClassifier  # å¯¼å…¥æŠ•ç¥¨åˆ†ç±»å™¨é›†æˆå·¥å…·
from sklearn.preprocessing import MaxAbsScaler  # å¯¼å…¥æœ€å¤§ç»å¯¹å€¼ç¼©æ”¾å™¨ï¼Œé€‚åˆç¨€ç–æ•°æ®
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # å¯¼å…¥è¯„ä¼°æŒ‡æ ‡
from scipy.sparse import hstack, csr_matrix  # å¯¼å…¥ç¨€ç–çŸ©é˜µåˆå¹¶å·¥å…·
import seaborn as sns  # å¯¼å…¥ Seaborn ç»Ÿè®¡ç»˜å›¾åº“
from wordcloud import WordCloud  # å¯¼å…¥è¯äº‘ç”Ÿæˆåº“
import warnings  # å¯¼å…¥è­¦å‘Šæ§åˆ¶æ¨¡å—

# å¿½ç•¥è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸é‡è¦è­¦å‘Šï¼ˆå¦‚è¿­ä»£æœªæ”¶æ•›è­¦å‘Šç­‰ï¼‰
warnings.filterwarnings('ignore')  # è®¾ç½®è­¦å‘Šè¿‡æ»¤å™¨ä¸ºå¿½ç•¥

# --- å¯è§†åŒ–å…¨å±€é…ç½® ---
# è®¾ç½®ä¸­æ–‡å­—ä½“ä¸ºé»‘ä½“ (SimHei)ï¼Œé˜²æ­¢ç”Ÿæˆçš„å›¾è¡¨ä¸­çš„ä¸­æ–‡æ˜¾ç¤ºä¸ºæ–¹æ¡†ä¹±ç 
plt.rcParams['font.sans-serif'] = ['SimHei']
# è§£å†³åæ ‡è½´ä¸­è´Ÿå· (-) æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
plt.rcParams['axes.unicode_minus'] = False
# è®¾ç½®ç»˜å›¾èƒŒæ™¯é£æ ¼ä¸ºç™½æ ¼ï¼Œå¹¶åŒæ­¥æŒ‡å®šå­—ä½“é…ç½®
sns.set_style('whitegrid', {'font.sans-serif': ['SimHei']})

# å®šä¹‰æ•°æ®é›†çš„æœ¬åœ°ç»å¯¹è·¯å¾„å­—å…¸
file_urls = {
    "train": r"C:\Users\Administrator\PycharmProjects\JupyterProject\data\train.csv",  # è®­ç»ƒé›†è·¯å¾„
    "validation": r"C:\Users\Administrator\PycharmProjects\JupyterProject\data\dev.csv",  # éªŒè¯é›†è·¯å¾„
    "test": r"C:\Users\Administrator\PycharmProjects\JupyterProject\data\test.csv"  # æµ‹è¯•é›†è·¯å¾„
}


def tokenize(text, stopwords):
    """
    æ–‡æœ¬åˆ†è¯ä¸æ¸…æ´—æ¨¡å—ï¼š
    1. æ­£åˆ™æ¸…æ´—ï¼šé€šè¿‡ [^\u4e00-\u9fff0-9a-zA-Z] è¿‡æ»¤æ‰æ‰€æœ‰æ ‡ç‚¹ç¬¦å·ã€ç‰¹æ®Šè¡¨æƒ…å’Œæ‚è´¨ã€‚
    2. æ·±åº¦åˆ†è¯ï¼šè°ƒç”¨ jieba åº“å°†è¿ç»­çš„ä¸­æ–‡åˆ‡åˆ†ä¸ºç¬¦åˆè¯­ä¹‰çš„è¯ç»„ã€‚
    3. åœç”¨è¯è¿‡æ»¤ï¼šå‰”é™¤â€œçš„â€ã€â€œäº†â€ã€â€œæ˜¯â€ç­‰é«˜é¢‘ä½†æ— å®é™…åˆ†ç±»æ„ä¹‰çš„è¯æ±‡ã€‚
    """
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†éä¸­æ–‡å­—ç¬¦ã€éæ•°å­—ã€éè‹±æ–‡å­—æ¯æ›¿æ¢ä¸ºç©ºæ ¼
    text = re.sub(r'[^\u4e00-\u9fff0-9a-zA-Z]', ' ', str(text))
    # ä½¿ç”¨ jieba è¿›è¡Œä¸­æ–‡ç²¾ç¡®æ¨¡å¼åˆ†è¯
    words = jieba.cut(text)
    # è¿‡æ»¤æ‰åœç”¨è¯è¡¨ä¸­çš„è¯æ±‡ä»¥åŠçº¯ç©ºç™½å­—ç¬¦ï¼Œå¹¶ç”¨ç©ºæ ¼æ‹¼æ¥
    return " ".join([w for w in words if w not in stopwords and w.strip()])


def get_stats(texts):
    """
    ç»Ÿè®¡ç‰¹å¾æå–æ¨¡å—ï¼š
    é™¤äº†æ–‡å­—è¯­ä¹‰ï¼Œåƒåœ¾çŸ­ä¿¡åœ¨å½¢æ€ä¸Šä¹Ÿæœ‰æ˜¾è‘—ç‰¹å¾ã€‚
    1. æ–‡æœ¬é•¿åº¦ï¼šåƒåœ¾çŸ­ä¿¡é€šå¸¸ä¸ºäº†åŒ…å«æ›´å¤šè¯±å¯¼å†…å®¹ï¼Œé•¿åº¦åé•¿ã€‚
    2. æ•°å­—å¯†åº¦ï¼šåƒåœ¾çŸ­ä¿¡å¸¸å«æœ‰ç”µè¯ã€QQå·ã€é‡‘é¢æˆ–æ—¥æœŸã€‚
    3. å…³é”®è¯è§¦å‘ï¼šç»Ÿè®¡â€˜çº¢åŒ…â€™ã€â€˜é“¾æ¥â€™ã€â€˜åŠ å¾®â€™ç­‰å¼ºè¯±å¯¼æ€§è¯æ±‡çš„å‡ºç°æ¬¡æ•°ã€‚
    """
    res = []  # åˆå§‹åŒ–ç»Ÿè®¡ç»“æœåˆ—è¡¨
    # å®šä¹‰åƒåœ¾çŸ­ä¿¡å¸¸è§çš„äººå·¥è¯†åˆ«å…³é”®è¯åº“
    keywords = ['å…è´¹', 'çº¢åŒ…', 'é“¾æ¥', 'åŠ å¾®', 'é€€è®¢', 'ä¸­å¥–', 'ç§¯åˆ†', 'å›T']
    for t in texts:  # éå†æ¯ä¸€æ¡çŸ­ä¿¡æ–‡æœ¬
        t = str(t)  # å¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²é˜²æ­¢ç©ºå€¼æŠ¥é”™
        # è®¡ç®—ï¼š1.æ€»é•¿åº¦ï¼›2.æ•°å­—ä¸²å‡ºç°çš„é¢‘æ¬¡ï¼›3.å…³é”®è¯åº“å‘½ä¸­çš„æ€»æ¬¡æ•°
        res.append([len(t), len(re.findall(r'\d+', t)), sum(t.count(w) for w in keywords)])
    return np.array(res)  # è¿”å› NumPy æ•°ç»„æ ¼å¼çš„ç»Ÿè®¡ç‰¹å¾çŸ©é˜µ


def main_process():
    # --- 1. æ•°æ®åŠ è½½ ---
    # åˆ†åˆ«ä»æœ¬åœ°è·¯å¾„è¯»å– CSV æ ¼å¼çš„æ•°æ®é›†æ–‡ä»¶
    df_train = pd.read_csv(file_urls["train"])  # åŠ è½½è®­ç»ƒæ•°æ®
    df_val = pd.read_csv(file_urls["validation"])  # åŠ è½½å¼€å‘/éªŒè¯æ•°æ®
    df_test = pd.read_csv(file_urls["test"])  # åŠ è½½åŸå§‹æµ‹è¯•æ•°æ®

    # å°†è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆå¹¶ï¼Œåˆ©ç”¨æ›´å¤šæ•°æ®è®­ç»ƒæ¨¡å‹ä»¥æå‡é²æ£’æ€§
    full_train_df = pd.concat([df_train, df_val], ignore_index=True)

    # æå–æ–‡æœ¬åˆ—ï¼Œå¡«å……ç¼ºå¤±å€¼ä¸ºå­—ç¬¦å¹¶è½¬ä¸º Python åˆ—è¡¨æ ¼å¼
    train_texts = full_train_df['text'].fillna("").astype(str).tolist()
    # æå–è®­ç»ƒæ ‡ç­¾åˆ—
    train_labels = full_train_df['label'].tolist()
    # æå–æµ‹è¯•æ–‡æœ¬åˆ—
    test_texts = df_test['text'].fillna("").astype(str).tolist()
    # æå–æµ‹è¯•æ ‡ç­¾åˆ—
    test_labels = df_test['label'].tolist()

    # --- 2. ç‰¹å¾å·¥ç¨‹ (Feature Engineering) ---
    print("æ­£åœ¨æå–ç‰¹å¾å¹¶åˆ†è¯...")  # æ‰“å°æ§åˆ¶å°è¿›åº¦æç¤º
    # å®šä¹‰åŸºç¡€åœç”¨è¯è¿‡æ»¤é›†åˆ
    stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€'}
    # å¯¹è®­ç»ƒé›†æ–‡æœ¬è¿›è¡Œæ¸…æ´—å’Œåˆ†è¯å¤„ç†
    train_cleaned = [tokenize(t, stopwords) for t in train_texts]
    # å¯¹æµ‹è¯•é›†æ–‡æœ¬è¿›è¡ŒåŒæ­¥æ¸…æ´—å’Œåˆ†è¯å¤„ç†
    test_cleaned = [tokenize(t, stopwords) for t in test_texts]

    # ç‰¹å¾Aï¼šè¯è¢‹ç‰¹å¾ (TF-IDF Word) - è®¾ç½® 1-2 è¯ç»„ç»„åˆï¼Œå–å‰ 8000 ä¸ªæœ€æ˜¾è‘—ç‰¹å¾
    tfidf_word = TfidfVectorizer(ngram_range=(1, 2), max_features=8000, sublinear_tf=True)
    X_train_word = tfidf_word.fit_transform(train_cleaned)  # å­¦ä¹ å¹¶è½¬æ¢è®­ç»ƒæ–‡æœ¬
    X_test_word = tfidf_word.transform(test_cleaned)  # è½¬æ¢æµ‹è¯•æ–‡æœ¬

    # ç‰¹å¾Bï¼šå­—ç¬¦ç‰¹å¾ (TF-IDF Char) - æŒ‰å­—ç¬¦åˆ‡å‰² 2-4 ç»„åˆï¼Œå‰ 4000 ç‰¹å¾ï¼Œé€‚åˆå‘ç°å¹²æ‰°å˜ä½“è¯
    tfidf_char = TfidfVectorizer(analyzer='char', ngram_range=(2, 4), max_features=4000, sublinear_tf=True)
    X_train_char = tfidf_char.fit_transform(train_cleaned)  # å­¦ä¹ å¹¶è½¬æ¢è®­ç»ƒå­—ç¬¦ç‰¹å¾
    X_test_char = tfidf_char.transform(test_cleaned)  # è½¬æ¢æµ‹è¯•å­—ç¬¦ç‰¹å¾

    # ç‰¹å¾Cï¼šå½¢æ€ç‰¹å¾ (Stats) - å¯¹é•¿åº¦ã€æ•°å­—ç­‰ç»Ÿè®¡å€¼è¿›è¡Œæœ€å¤§ç»å¯¹å€¼ç¼©æ”¾ï¼ˆä¿æŒç¨€ç–æ€§ï¼‰
    X_train_stats = MaxAbsScaler().fit_transform(get_stats(train_texts))
    # ä½¿ç”¨è®­ç»ƒé›†çš„ç¼©æ”¾æ ‡å‡†æ¥è½¬æ¢æµ‹è¯•é›†çš„ç»Ÿè®¡ç‰¹å¾ï¼Œä¿æŒä¸€è‡´æ€§
    X_test_stats = MaxAbsScaler().fit(get_stats(train_texts)).transform(get_stats(test_texts))
    # å®ä¾‹åŒ–å¹¶ä¿å­˜ç¼©æ”¾å™¨ï¼Œä»¥ä¾¿åœ¨éƒ¨ç½²åº”ç”¨ä¸­å¯¹å•æ¡è¾“å…¥è¿›è¡Œå¤„ç†
    scaler = MaxAbsScaler().fit(get_stats(train_texts))

    # æ··åˆç‰¹å¾ç»„åˆï¼šå°†è¯ TF-IDFã€å­— TF-IDFã€ç»Ÿè®¡ç‰¹å¾æ¨ªå‘æ‹¼æ¥ä¸ºè¶…å¤§ç‰¹å¾å‘é‡
    X_train = hstack([X_train_word, X_train_char, csr_matrix(X_train_stats)])
    X_test = hstack([X_test_word, X_test_char, csr_matrix(X_test_stats)])

    # --- 3. é›†æˆæ¨¡å‹è®¾è®¡ (Ensemble Modeling) ---
    print("å¼€å§‹è®­ç»ƒ (è½¯æŠ•ç¥¨æ¨¡å¼ï¼Œè®¡ç®—æ¦‚ç‡ä¸­)...")  # æ‰“å°è¿›åº¦
    # è½¯æŠ•ç¥¨åŸç†ï¼šå°†å¤šä¸ªåˆ†ç±»å™¨çš„æ¦‚ç‡è¾“å‡ºå–å‡å€¼/åŠ æƒå¹³å‡ï¼Œä½œä¸ºæœ€ç»ˆåˆ¤å®šä¾æ®ã€‚
    ensemble = VotingClassifier(
        estimators=[
            # é€»è¾‘å›å½’ï¼šæ“…é•¿æ•æ‰å…¨å±€è¯é¢‘åˆ†å¸ƒï¼ŒC=3.0 å¢åŠ å¯¹é‡è¦ç‰¹å¾çš„å…³æ³¨åº¦
            ('lr', LogisticRegression(C=3.0, max_iter=1000, class_weight='balanced')),
            # æ”¯æŒå‘é‡æœºï¼šå¯»æ‰¾æœ€ä½³è¶…å¹³é¢ï¼Œprobability=True å¼€å¯æ¦‚ç‡é¢„æµ‹ï¼Œè€—æ—¶ä½†ç²¾ç¡®
            ('svc', SVC(kernel='linear', C=1.0, probability=True, class_weight='balanced')),
            # è¡¥é›†æœ´ç´ è´å¶æ–¯ï¼šé’ˆå¯¹éå¹³è¡¡è¯­æ–™ä¼˜åŒ–çš„æœ´ç´ è´å¶æ–¯ï¼Œalpha=0.1 å‡å¼±å¹³æ»‘
            ('cnb', ComplementNB(alpha=0.1))
        ],
        voting='soft'  # å¼€å¯è½¯æŠ•ç¥¨ï¼šæ¨¡å‹å°†è¿”å›æ¦‚ç‡ç™¾åˆ†æ¯”è€Œéç¡¬ç±»åˆ«æ ‡ç­¾
    )
    ensemble.fit(X_train, train_labels)  # ä½¿ç”¨æ‹¼æ¥åçš„ç»¼åˆç‰¹å¾çŸ©é˜µè®­ç»ƒé›†æˆæ¨¡å‹

    # --- 4. æ€§èƒ½è¯„ä¼° ---
    y_pred = ensemble.predict(X_test)  # åœ¨æµ‹è¯•é›†ä¸Šç”Ÿæˆé¢„æµ‹ç»“æœ
    print(f"\nğŸ† æœ€ç»ˆæµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy_score(test_labels, y_pred):.4f}")  # æ‰“å°å‡†ç¡®ç‡
    # æ‰“å°è¯¦ç»†çš„ç²¾ç¡®ç‡ (P)ã€å¬å›ç‡ (R) å’Œ F1 å€¼è¯„ä¼°æŠ¥å‘Š
    print(classification_report(test_labels, y_pred, target_names=['æ­£å¸¸', 'åƒåœ¾']))

    # --- 5. æ¨¡å‹æŒä¹…åŒ– ---
    print("æ­£åœ¨ä¿å­˜æ¨¡å‹å’Œå‚æ•°...")  # æ‰“å°æç¤º
    joblib.dump(ensemble, 'spam_model.pkl')  # ä¿å­˜é›†æˆæ¨¡å‹å¯¹è±¡
    joblib.dump(tfidf_word, 'tfidf_word.pkl')  # ä¿å­˜è¯ç‰¹å¾å‘é‡åŒ–å™¨
    joblib.dump(tfidf_char, 'tfidf_char.pkl')  # ä¿å­˜å­—ç‰¹å¾å‘é‡åŒ–å™¨
    joblib.dump(scaler, 'scaler.pkl')  # ä¿å­˜æ•°å€¼ç¼©æ”¾å™¨

    # --- 6. è‡ªåŠ¨åŒ–å¯è§†åŒ–åˆ†æ ---
    print("æ­£åœ¨ç”Ÿæˆå¹¶å†™å…¥å¯è§†åŒ–å›¾è¡¨...")  # æ‰“å°æç¤º
    plt.figure(figsize=(18, 12))  # åˆ›å»ºä¸€ä¸ªå®½ 18 è‹±å¯¸ã€é«˜ 12 è‹±å¯¸çš„å¤§ç”»å¸ƒ
    plt.subplots_adjust(hspace=0.4, wspace=0.3)  # è°ƒæ•´å­å›¾é—´çš„å‚ç›´å’Œæ°´å¹³é—´è·

    # 6.1 æ··æ·†çŸ©é˜µï¼šç”¨äºè§‚å¯Ÿå“ªäº›æ­£å¸¸çŸ­ä¿¡è¢«è¯¯åˆ¤æˆäº†åƒåœ¾ï¼ˆè¯¯æŠ¥ç‡ï¼‰
    ax1 = plt.subplot(2, 2, 1)  # å®šä½åˆ° 2x2 ç”»å¸ƒçš„ç¬¬ 1 ä¸ªä½ç½®
    cm = confusion_matrix(test_labels, y_pred)  # è®¡ç®—æ··æ·†çŸ©é˜µæ•°å€¼
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',  # ç»˜åˆ¶çƒ­åŠ›å›¾å¹¶æ˜¾ç¤ºæ•°å€¼
                xticklabels=['æ­£å¸¸', 'åƒåœ¾'], yticklabels=['æ­£å¸¸', 'åƒåœ¾'], ax=ax1)
    ax1.set_title('æ¨¡å‹æ··æ·†çŸ©é˜µ (Confusion Matrix)')  # è®¾ç½®æ ‡é¢˜

    # 6.2 è¯äº‘å›¾ï¼šå±•ç¤ºåƒåœ¾çŸ­ä¿¡ä¸­å‡ºç°é¢‘ç‡æœ€é«˜çš„è¯±å¯¼æ€§æ•æ„Ÿè¯
    font_path = r"C:\Windows\Fonts\simhei.ttf"  # æŒ‡å®šç³»ç»Ÿå­—ä½“è·¯å¾„ä»¥æ”¯æŒè¯äº‘ä¸­æ–‡
    # ç­›é€‰å‡ºè¢«æ ‡è®°ä¸ºåƒåœ¾çŸ­ä¿¡çš„æ–‡æœ¬å¹¶æ‹¼æ¥æˆä¸€ä¸ªå·¨å‹å­—ç¬¦ä¸²
    spam_text = " ".join([test_cleaned[i] for i in range(len(test_labels)) if test_labels[i] == 1])
    ax2 = plt.subplot(2, 2, 2)  # å®šä½åˆ°ç”»å¸ƒç¬¬ 2 ä¸ªä½ç½®
    if spam_text.strip():  # å¦‚æœåƒåœ¾æ–‡æœ¬ä¸ä¸ºç©º
        # ç”Ÿæˆè¯äº‘å¯¹è±¡ï¼Œè®¾ç½®èƒŒæ™¯è‰²ã€å®½é«˜åŠå­—ä½“
        wc = WordCloud(font_path=font_path, background_color='white', width=400, height=300).generate(spam_text)
        ax2.imshow(wc, interpolation='bilinear')  # æ¸²æŸ“è¯äº‘å›¾ç‰‡
    ax2.axis('off')  # å…³é—­è¯äº‘å›¾çš„åæ ‡è½´æ˜¾ç¤º
    ax2.set_title('åƒåœ¾çŸ­ä¿¡é«˜é¢‘å…³é”®è¯äº‘')  # è®¾ç½®æ ‡é¢˜

    # 6.3 é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå›¾ï¼šå±•ç¤ºé›†æˆæ¨¡å‹å¯¹åˆ†ç±»åˆ¤å®šçš„ä¿¡å¿ƒåˆ†å¸ƒï¼ˆè¶Šé è¿‘ 0 æˆ– 1 è¶Šè‡ªä¿¡ï¼‰
    ax3 = plt.subplot(2, 2, 3)  # å®šä½åˆ°ç”»å¸ƒç¬¬ 3 ä¸ªä½ç½®
    y_proba = ensemble.predict_proba(X_test)[:, 1]  # è·å–åˆ¤å®šä¸ºâ€œåƒåœ¾çŸ­ä¿¡â€çš„æ¦‚ç‡å€¼
    sns.histplot(y_proba, bins=20, kde=True, color='red', ax=ax3)  # ç»˜åˆ¶å¸¦æ ¸å¯†åº¦ä¼°è®¡çš„ç›´æ–¹å›¾
    ax3.set_title('é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ (ç½®ä¿¡åº¦åˆ†æ)')  # è®¾ç½®æ ‡é¢˜
    ax3.set_xlabel('åˆ¤å®šä¸ºâ€œåƒåœ¾â€çš„ç½®ä¿¡å¾—åˆ†')  # è®¾ç½®æ¨ªè½´æ ‡ç­¾

    # 6.4 é•¿åº¦å¯¹æ¯”å›¾ï¼šç›´è§‚å±•ç¤ºåƒåœ¾çŸ­ä¿¡ä¸æ­£å¸¸çŸ­ä¿¡åœ¨æ–‡æœ¬é•¿åº¦ä¸Šçš„åˆ†å¸ƒå·®å¼‚
    ax4 = plt.subplot(2, 2, 4)  # å®šä½åˆ°ç”»å¸ƒç¬¬ 4 ä¸ªä½ç½®
    # åˆ›å»ºä¸´æ—¶æ•°æ®å¸§ç”¨äºç»˜å›¾
    df_plot = pd.DataFrame({'ç±»åˆ«': [('åƒåœ¾' if l == 1 else 'æ­£å¸¸') for l in test_labels],
                            'é•¿åº¦': [len(t) for t in test_texts]})
    # ç»˜åˆ¶ç®±çº¿å›¾ï¼Œè¿‡æ»¤æ‰é•¿åº¦è¶…è¿‡ 300 çš„å¼‚å¸¸é•¿æ–‡æœ¬ä»¥ä¾¿è§‚å¯Ÿæ ¸å¿ƒåŒºé—´
    sns.boxplot(x='ç±»åˆ«', y='é•¿åº¦', data=df_plot[df_plot['é•¿åº¦'] < 300], palette="Set2", ax=ax4)
    ax4.set_title('çŸ­ä¿¡é•¿åº¦åˆ†å¸ƒå¯¹æ¯”ç®±çº¿å›¾')  # è®¾ç½®æ ‡é¢˜

    # --- 7. å›¾ç‰‡å¼ºåˆ¶ä¿å­˜ ---
    save_path = os.path.join(os.getcwd(), 'model_results.png')  # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•å¹¶å®šä¹‰å›¾ç‰‡æ–‡ä»¶å
    # å°†ç”»å¸ƒä¿å­˜ä¸º PNG å›¾ç‰‡ï¼Œè®¾ç½®åˆ†è¾¨ç‡ã€ç´§å‡‘å¸ƒå±€åŠç™½è‰²èƒŒæ™¯
    plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')
    plt.close('all')  # å…³é—­å†…å­˜ä¸­æ‰€æœ‰çš„ç»˜å›¾å¯¹è±¡ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼

    print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜è‡³: {save_path}")  # æ§åˆ¶å°æ‰“å°å®Œæˆè·¯å¾„æç¤º


if __name__ == '__main__':
    main_process()  # å¦‚æœæ˜¯ç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼Œåˆ™å¯åŠ¨ä¸»æµç¨‹å‡½æ•°